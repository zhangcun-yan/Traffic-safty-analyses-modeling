{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangcun-yan/Traffic-safty-analyses-modeling/blob/master/Mutl_denoise_compare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7b0d42c",
      "metadata": {
        "id": "f7b0d42c"
      },
      "source": [
        "## 首先导入相关的库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a588263",
      "metadata": {
        "id": "5a588263"
      },
      "outputs": [],
      "source": [
        "import scipy.io as scio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import matplotlib.mlab as mlab\n",
        "# 数据分析相关包\n",
        "import seaborn as sns\n",
        "from numpy.random import randn\n",
        "import matplotlib as mpl\n",
        "from scipy import stats\n",
        "from matplotlib.patches import Polygon\n",
        "import math\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 运行后出现URL，点击获得你的认证码\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7193E9ipOdr0"
      },
      "id": "7193E9ipOdr0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 列出你云盘里有啥\n",
        "!ls \"/content/drive//My Drive/data_set\""
      ],
      "metadata": {
        "id": "2xBwJiF5PDw7"
      },
      "id": "2xBwJiF5PDw7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9760fdc9",
      "metadata": {
        "id": "9760fdc9"
      },
      "outputs": [],
      "source": [
        "mask_rcnn_orginal_total = r'/content/drive/My Drive/data_set/Mask_RCNN/wavelet_filter_Mask.csv'\n",
        "mask_rcnn_orginal_remove = r'/content/drive/My Drive/data_set/Mask_RCNN/Mask_remove_wavel0926.csv'\n",
        "mask_rcnn_wavelet = r'/content/drive/My Drive/data_set/Mask_RCNN/Mask_wavel0926.csv'\n",
        "\n",
        "mask_rcnn_orginal_total = pd.read_csv(mask_rcnn_orginal_total)\n",
        "mask_rcnn_orginal_total = pd.DataFrame(mask_rcnn_orginal_total)\n",
        "mask_rcnn_orginal_total = mask_rcnn_orginal_total[mask_rcnn_orginal_total['Vehicle_type']==22]\n",
        "\n",
        "mask_rcnn_orginal_remove = pd.read_csv(mask_rcnn_orginal_remove)\n",
        "mask_rcnn_orginal_remove = pd.DataFrame(mask_rcnn_orginal_remove)\n",
        "\n",
        "mask_rcnn_wavelet = pd.read_csv(mask_rcnn_wavelet)\n",
        "mask_rcnn_wavelet = pd.DataFrame(mask_rcnn_wavelet)\n",
        "\n",
        "plt.scatter(mask_rcnn_orginal_total['World_x'],mask_rcnn_orginal_total['World_y'],color='k',s=2)\n",
        "plt.scatter(mask_rcnn_orginal_remove['World_x'],mask_rcnn_orginal_remove['World_y'],color='r',s=2)\n",
        "# plt.scatter(mask_rcnn_remove['World_x'],mask_rcnn_remove['World_y'],color='r',s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19e290d6",
      "metadata": {
        "id": "19e290d6"
      },
      "outputs": [],
      "source": [
        "Yolov7_orginal0 = r\"/content/drive/My Drive/data_set/Yolodata/wavelet_filter_yolov7.csv\"\n",
        "Yolov7_orginal = r\"/content/drive/My Drive/data_set/Yolodata/Yolov7_wavel0926.csv\"\n",
        "Yolov7_denoise = r\"/content/drive/My Drive/data_set/Yolodata/Yolo_remove_wavel0926.csv\"\n",
        "Yolov7_orginal0 = pd.read_csv(Yolov7_orginal0)\n",
        "Yolov7_orginal0 = pd.DataFrame(Yolov7_orginal0)\n",
        "Yolov7_orginal0 = Yolov7_orginal0[Yolov7_orginal0['Vehicle_type']==22]\n",
        "\n",
        "Yolov7_orginal = pd.read_csv(Yolov7_orginal)\n",
        "Yolov7_orginal = pd.DataFrame(Yolov7_orginal)\n",
        "\n",
        "Yolov7_denoise = pd.read_csv(Yolov7_denoise)\n",
        "Yolov7_denoise = pd.DataFrame(Yolov7_denoise)\n",
        "\n",
        "plt.scatter(Yolov7_orginal0['World_x'],Yolov7_orginal0['World_y'],color='r',s=2)\n",
        "# plt.scatter(Yolov7_orginal['World_x'],Yolov7_orginal['World_y'],color='r',s=2)\n",
        "plt.scatter(Yolov7_denoise['World_x'],Yolov7_denoise['World_y'],color='g',s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a2ed484",
      "metadata": {
        "id": "1a2ed484"
      },
      "source": [
        "## 导入初始数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e08e2a0",
      "metadata": {
        "id": "1e08e2a0"
      },
      "outputs": [],
      "source": [
        "YOLOV7_ogrinal_only_T = r\"/content/drive/My Drive/data_set/Yolodata/RAW_S_T_TRAJ.xlsx\"\n",
        "YOLOV7_ogrinal_only_T = pd.read_excel(YOLOV7_ogrinal_only_T)\n",
        "YOLOV7_ogrinal_only_T = pd.DataFrame(YOLOV7_ogrinal_only_T)\n",
        "YOLOV7_ogrinal_only_T = YOLOV7_ogrinal_only_T[YOLOV7_ogrinal_only_T['Vehicle_type']==22]\n",
        "plt.scatter(YOLOV7_ogrinal_only_T['World_x'],YOLOV7_ogrinal_only_T['World_y'],color='g',s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8fbabb2",
      "metadata": {
        "id": "d8fbabb2"
      },
      "source": [
        "## 导入kalman滤波后的数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37909be",
      "metadata": {
        "id": "a37909be"
      },
      "outputs": [],
      "source": [
        "YOLOV7_ogrinal = r\"/content/drive/My Drive/data_set/Yolodata/S_raw.csv\"\n",
        "YOLOV7_ogrinal = pd.read_csv(YOLOV7_ogrinal)\n",
        "YOLOV7_ogrinal = pd.DataFrame(YOLOV7_ogrinal)\n",
        "YOLOV7_ogrinal = YOLOV7_ogrinal[YOLOV7_ogrinal['Vehicle_type']==22]\n",
        "plt.scatter(YOLOV7_ogrinal['World_x'],YOLOV7_ogrinal['World_y'],color='g',s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701c7aaa",
      "metadata": {
        "id": "701c7aaa"
      },
      "outputs": [],
      "source": [
        "YOLOV7_kalman = r\"/content/drive/My Drive/data_set/Yolodata/S_Kalman.csv\"\n",
        "YOLOV7_kalman = pd.read_csv(YOLOV7_kalman)\n",
        "YOLOV7_kalman = pd.DataFrame(YOLOV7_kalman)\n",
        "YOLOV7_kalman = YOLOV7_kalman[YOLOV7_kalman['Vehicle_type']==22]\n",
        "plt.scatter(YOLOV7_kalman['World_x'],YOLOV7_kalman['World_y'],color='g',s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae6064d0",
      "metadata": {
        "id": "ae6064d0"
      },
      "source": [
        "## 计算轨迹的相关参数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca5c6c1",
      "metadata": {
        "id": "dca5c6c1"
      },
      "outputs": [],
      "source": [
        "# 计算轨迹中的速度/加速度/加加速度\n",
        "def XY(groundtraj,caompartraj):\n",
        "    g_World_x = np.array(groundtraj['World_x'].astype(float))\n",
        "    g_World_y = np.array(groundtraj['World_y'].astype(float))\n",
        "    com_World_x = np.array(caompartraj['World_x'].astype(float))\n",
        "    com_World_y = np.array(caompartraj['World_y'].astype(float))    \n",
        "    return g_World_x,g_World_y,com_World_x,com_World_y\n",
        "\n",
        "def Velocity(trajdata):\n",
        "    \"\"\"recalculate the velocity, the velocity contain the X_velocity and Y-velocity and the speed\"\"\"\n",
        "    \"\"\"定义初速度为0\"\"\"\n",
        "    len_x = trajdata.shape[0]\n",
        "    Wordld_x = np.array(trajdata.World_x)\n",
        "    Wordld_y = np.array(trajdata.World_y)\n",
        "    velocity_x = (Wordld_x[1:len_x]-Wordld_x[0:len_x-1])/0.04\n",
        "    velocity_y = (Wordld_y[1:len_x]-Wordld_y[0:len_x-1])/0.04\n",
        "    velocity_x = np.insert(velocity_x,0,0)\n",
        "    velocity_y = np.insert(velocity_y,0,0)\n",
        "    return velocity_x,velocity_y\n",
        "\n",
        "def Accelection(trajdata):\n",
        "    \"\"\"定结束时刻的加速度为0\"\"\"\n",
        "    # print(trajdata)\n",
        "    len_x = trajdata.shape[0]\n",
        "    velocity_x = np.array(trajdata.speed_x)\n",
        "    velocity_y = np.array(trajdata.speed_y)\n",
        "    accelection_x = (velocity_x[1:len_x]-velocity_x[0:len_x-1])/0.04\n",
        "    accelection_y = (velocity_y[1:len_x]-velocity_y[0:len_x-1])/0.04\n",
        "    accelection_x = np.insert(accelection_x,0,0)\n",
        "    accelection_y = np.insert(accelection_y,0,0)\n",
        "    accelection_x[1] = 0\n",
        "    accelection_y[1] = 0\n",
        "    return accelection_x,accelection_y\n",
        "\n",
        "def Aclculate_Jerk(trajdata):\n",
        "    \"\"\"计算急动度\"\"\" \n",
        "    len_x = trajdata.acc_x.shape[0]\n",
        "    acc_x = np.array(trajdata.acc_x)\n",
        "    acc_y = np.array(trajdata.acc_y)\n",
        "    JJerk_x = (acc_x[1:len_x] - acc_x[0:(len_x-1)])/0.04\n",
        "    JJerk_y = (acc_y[1:len_x] - acc_y[0:(len_x-1)])/0.04\n",
        "    JJerk_x = np.insert(JJerk_x,0,0)\n",
        "    JJerk_y = np.insert(JJerk_y,0,0)\n",
        "    JJerk_x[2] = 0\n",
        "    JJerk_y[2] = 0\n",
        "    return JJerk_x,JJerk_y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29595cc2",
      "metadata": {
        "id": "29595cc2"
      },
      "source": [
        "## 绘制轨迹图像"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e04ad1e4",
      "metadata": {
        "id": "e04ad1e4"
      },
      "outputs": [],
      "source": [
        "def Trajmap(Data,Orginal,mothod1):\n",
        "    # 设置图框大小\n",
        "    plt.Figure([2000,2000],dpi=20)\n",
        "    fig = plt.figure(figsize=(10,5))   \n",
        "    sns.despine(left=True)    \n",
        "    #绘制轨迹\n",
        "    plt.scatter(Data['World_x'],Data['World_y'],c='red',label='Ground trajectory')\n",
        "    plt.scatter(Orginal['World_x'],Orginal['World_y'],c='k',label='Yolov7 Orginal trajectory')\n",
        "    plt.scatter(mothod1['World_x'],mothod1['World_y'],c='blue',label='SFPF Remove Noise')\n",
        "#     plt.scatter(mothod2['World_x'],mothod2['World_y'],c='green',label='Wavelet Remove Noise')\n",
        "    plt.legend()\n",
        "    plt.legend(loc=\"upper right\",fontsize=12)   #设置图例字体大小\n",
        "\n",
        "    plt.xlabel('X (m))',fontsize=16)     #设置x轴名称\n",
        "    plt.legend(loc=\"upper right\",fontsize=16)   #设置图例字体大小\n",
        "    # 坐标尺\n",
        "    plt.xticks(fontsize=12,rotation=0)\n",
        "    plt.yticks(fontsize=12)\n",
        "    # 坐标名称\n",
        "    plt.rcParams.update({\"font.size\":16})\n",
        "#     plt.title(\"The trajectory\",fontsize=16)\n",
        "    plt.ylabel('Y (m)',fontsize=16)     #设置y轴名称  \n",
        "    plt.grid()\n",
        "    plt.show() \n",
        "# 多维轨迹显示图\n",
        "# 绘制时空分布图\n",
        "def Stmap(data):\n",
        "    # 计算车辆个数\n",
        "    vehids = pd.unique(data[\"Object_id\"])\n",
        "    # 设置图框大小\n",
        "    plt.Figure([2200,2200],dpi=1000)\n",
        "    for vehid in vehids:\n",
        "        # 获取第vehid辆车的轨迹\n",
        "        traj_vehid = data[data.Object_id == vehid]\n",
        "        # 获取x方向轨迹点\n",
        "        x = traj_vehid['World_x']\n",
        "        y = traj_vehid['World_y']\n",
        "        v = traj_vehid['speed_xy']\n",
        "        norm = matplotlib.colors.Normalize(vmin=0, vmax=50)\n",
        "        # 绘制散点图\n",
        "        ax = plt.scatter(x,y, marker = '.', s=1, c=v, cmap='jet_r', norm = norm)\n",
        "    plt.clim(0, 25)\n",
        "    plt.colorbar()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee9c53f",
      "metadata": {
        "id": "4ee9c53f"
      },
      "source": [
        "## 定义滤波降噪函数"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16fec46d",
      "metadata": {
        "id": "16fec46d"
      },
      "source": [
        "### 高斯平滑过程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c947c373",
      "metadata": {
        "id": "c947c373"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import scipy.signal\n",
        "from scipy import *\n",
        "import copy\n",
        "\n",
        "def gaussian(t, fwhm):\n",
        "    return np.exp(-(4*np.log(2)*t**2)/fwhm**2)\n",
        "\n",
        "srate = 1000 #Hz\n",
        "time  = np.arange(0,3,1/srate)\n",
        "n     = len(time)\n",
        "p     = 15 # poles for random interpolation\n",
        "\n",
        "#noise level, measured in standard deviations\n",
        "noiseamp = 5\n",
        "\n",
        "# amplitude modular and noise level\n",
        "ampl   = np.interp(np.linspace(1,p,n), np.arange(0,p), np.random.rand(p)*30)\n",
        "noise  = noiseamp * np.random.randn(n)\n",
        "signal = ampl + noise\n",
        "\n",
        "fwhm    = 25 # in ms\n",
        "\n",
        "# normalized time vector in ms\n",
        "k       = 100\n",
        "gtime   = 1000*np.arange(-k, k)/srate\n",
        "\n",
        "# Gaussian window\n",
        "gauswin = gaussian(gtime, fwhm)\n",
        "\n",
        "# Compute empirical FWHM\n",
        "pstPeakHalf = k + np.argmin( (gauswin[k:] -.5)**2 )\n",
        "prePeakHalf = np.argmin( (gauswin -.5)**2 )\n",
        "\n",
        "empFWHM = gtime[pstPeakHalf] - gtime[prePeakHalf]\n",
        "\n",
        "# shouw the Gausian\n",
        "# plt.plot(gtime, gauswin, 'ko-')\n",
        "# plt.plot([gtime[prePeakHalf],gtime[pstPeakHalf]],[gauswin[prePeakHalf],gauswin[pstPeakHalf]],'m')\n",
        "# Normalize Gaussian to unit energy\n",
        "gauswin = gauswin/np.sum(gauswin)\n",
        "# #title\n",
        "# plt.xlabel('Time (ms)')\n",
        "# plt.ylabel('Gain')\n",
        "# plt.show()\n",
        "\n",
        "# initialize filtered signal vector\n",
        "filtsigG = copy.deepcopy(signal)\n",
        "\n",
        "# implement the running mean filter\n",
        "for i in range(k+1, n-k-1):\n",
        "    filtsigG[i] = np.sum(signal[i-k:i+k] * gauswin)\n",
        "    \n",
        "plt.plot(time, signal, 'r', label='Original')\n",
        "plt.plot(time, filtsigG, 'k', label='Gaussian-filtered')\n",
        "\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('amp. (a.u)')\n",
        "plt.legend()\n",
        "plt.title('Gaussian smothing filter')\n",
        "plt.show()\n",
        "\n",
        "# filtsigMean = copy.deepcopy(signal)\n",
        "\n",
        "# mk = 20\n",
        "# for i in range(mk+1, n-mk-1):\n",
        "#     filtsigMean[i] = np.mean(signal[i-mk:i+mk])\n",
        "# plt.plot(time,signal,'r',label='Original')    \n",
        "# plt.plot(time, filtsigMean, 'b', label='Running mean')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a653e90c",
      "metadata": {
        "id": "a653e90c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "\n",
        "# Generate noisy trajectory data\n",
        "# n_samples = 200\n",
        "# X = np.linspace(0, 10, n_samples)\n",
        "# y = np.sin(X) + 0.2*np.random.randn(n_samples)\n",
        "\n",
        "def Denoise_GP(trajectory_data):\n",
        "    T = trajectory_data['frame_id'].values\n",
        "    X = trajectory_data['World_x']\n",
        "    y = trajectory_data['World_y']\n",
        "    # Define the Gaussian Process model\n",
        "    kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
        "\n",
        "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
        "    # Train the GP model\n",
        "    gp.fit(T.reshape(-1, 1), y)\n",
        "    gp.fit(T.reshape(-1,1),X)\n",
        "    # Predict the denoised trajectory\n",
        "    T_new = np.linspace(T[0], T[-1], len(T))\n",
        "    y_mean, y_std = gp.predict(T_new.reshape(-1, 1), return_std=True)\n",
        "    X_mean, X_std = gp.predict(T_new.reshape(-1, 1), return_std=True)\n",
        "    return T, y,T_new, y_mean,y_std,X_mean, X_std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "632bcb7b",
      "metadata": {
        "id": "632bcb7b"
      },
      "source": [
        "## Hampel Filter 降噪"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81326659",
      "metadata": {
        "id": "81326659"
      },
      "outputs": [],
      "source": [
        "# Define the Hampel filter function\n",
        "def hampel_filter(y, window_size, threshold):\n",
        "    n = y.shape[0]\n",
        "    y_filtered = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        lower = max(0, i - window_size // 2)\n",
        "        upper = min(n, i + window_size // 2 + 1)\n",
        "        median = np.median(y[lower:upper])\n",
        "        mad = np.median(np.abs(y[lower:upper] - median))\n",
        "        if np.abs(y[i] - median) > threshold * mad:\n",
        "            y_filtered[i] = median\n",
        "        else:\n",
        "            y_filtered[i] = y[i]\n",
        "    return y_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c80ea6e",
      "metadata": {
        "id": "5c80ea6e"
      },
      "source": [
        "## 小波变换"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d26c1f",
      "metadata": {
        "id": "90d26c1f"
      },
      "outputs": [],
      "source": [
        "#  模块调用\n",
        "# 小波变换\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pywt \n",
        "\n",
        "#封装成函数\n",
        "def sgn(num):\n",
        "    if(num > 0.0):\n",
        "        return 1.0\n",
        "    elif(num == 0.0):\n",
        "        return 0.0\n",
        "    else:\n",
        "        return -1.0\n",
        "def wavelet_noising(new_df):\n",
        "    data = new_df\n",
        "    data = data.values.T.tolist()  # 将np.ndarray()转为列表\n",
        "    w = pywt.Wavelet('sym8')#选择sym8小波基\n",
        "    [ca5, cd5, cd4, cd3, cd2, cd1] = pywt.wavedec(data, w, level=5)  # 5层小波分解\n",
        "    length1 = len(cd1)\n",
        "    length0 = len(data)\n",
        "    Cd1 = np.array(cd1)\n",
        "    abs_cd1 = np.abs(Cd1)\n",
        "    median_cd1 = np.median(abs_cd1)\n",
        "    sigma = (1.0 / 0.6745) * median_cd1\n",
        "    lamda = sigma * math.sqrt(2.0 * math.log(float(length0 ), math.e))#固定阈值计算\n",
        "    usecoeffs = []\n",
        "    usecoeffs.append(ca5)  # 向列表末尾添加对象\n",
        "    #软硬阈值折中的方法\n",
        "    a = 0.5\n",
        "    for k in range(length1):\n",
        "        if (abs(cd1[k]) >= lamda):\n",
        "            cd1[k] = sgn(cd1[k]) * (abs(cd1[k]) - a * lamda)\n",
        "        else:\n",
        "            cd1[k] = 0.0\n",
        "    length2 = len(cd2)\n",
        "    for k in range(length2):\n",
        "        if (abs(cd2[k]) >= lamda):\n",
        "            cd2[k] = sgn(cd2[k]) * (abs(cd2[k]) - a * lamda)\n",
        "        else:\n",
        "            cd2[k] = 0.0\n",
        "    length3 = len(cd3)\n",
        "    for k in range(length3):\n",
        "        if (abs(cd3[k]) >= lamda):\n",
        "            cd3[k] = sgn(cd3[k]) * (abs(cd3[k]) - a * lamda)\n",
        "        else:\n",
        "            cd3[k] = 0.0\n",
        "    length4 = len(cd4)\n",
        "    for k in range(length4):\n",
        "        if (abs(cd4[k]) >= lamda):\n",
        "            cd4[k] = sgn(cd4[k]) * (abs(cd4[k]) - a * lamda)\n",
        "        else:\n",
        "            cd4[k] = 0.0\n",
        "\n",
        "    length5 = len(cd5)\n",
        "    for k in range(length5):\n",
        "        if (abs(cd5[k]) >= lamda):\n",
        "            cd5[k] = sgn(cd5[k]) * (abs(cd5[k]) - a * lamda)\n",
        "        else:\n",
        "            cd5[k] = 0.0\n",
        "    usecoeffs.append(cd5)\n",
        "    usecoeffs.append(cd4)\n",
        "    usecoeffs.append(cd3)\n",
        "    usecoeffs.append(cd2)\n",
        "    usecoeffs.append(cd1)\n",
        "    recoeffs = pywt.waverec(usecoeffs, w)#信号重构\n",
        "    return recoeffs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33156890",
      "metadata": {
        "id": "33156890"
      },
      "source": [
        "##penalized denoising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335d555e",
      "metadata": {
        "id": "335d555e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import spdiags\n",
        "from scipy.sparse.linalg import spsolve\n",
        "\n",
        "def Denoise_penalized(trajectory,alpha):\n",
        "\n",
        "    n_samples = len(trajectory)\n",
        "    n_drones = 1\n",
        "    Y = np.zeros((n_samples, n_drones))\n",
        "\n",
        "    for i in range(0,1,n_samples):\n",
        "        Y[:, i] = trajectory[i]\n",
        "    # Define the penalty matrix\n",
        "    \n",
        "    d = np.ones(n_samples)\n",
        "    D = spdiags([-d, 2*d, -d], [-1, 0, 1], n_samples, n_samples)\n",
        "    n_drones = 1\n",
        "    # Set the regularization parameter    \n",
        "    # Denoise the trajectory data with penalized denoising\n",
        "    Y_denoised = np.zeros((n_samples, n_drones))\n",
        "    \n",
        "    for i in range(n_drones):\n",
        "        Y_denoised[:, i] = spsolve(D + alpha*np.eye(n_samples), Y[:, i])\n",
        "    return Y_denoised"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laplace 变换降噪"
      ],
      "metadata": {
        "id": "3sBhk25JEyJH"
      },
      "id": "3sBhk25JEyJH"
    },
    {
      "cell_type": "code",
      "source": [
        "# 调所需的包\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import spdiags\n",
        "from scipy.sparse.linalg import spsolve\n",
        "\n",
        "# 定义拉普拉斯降噪函数，函数是单独对X或者Y进行降噪\n",
        "def Laplacian_denoise(trajectory):\n",
        "#     trajectory_data = trajectory_data.values\n",
        "    n_samples = len(trajectory)   \n",
        "    # Define the Laplacian filter\n",
        "    d = np.ones(n_samples)\n",
        "    D = spdiags([-d, 2*d, -d], [-1, 0, 1], n_samples, n_samples)\n",
        "    # Set the regularization parameter\n",
        "    alpha = 1\n",
        "    # Denoise the trajectory data with Laplacian filtering\n",
        "    y_denoised = spsolve(D + alpha*np.eye(n_samples), trajectory)\n",
        "    return y_denoised"
      ],
      "metadata": {
        "id": "wuLRiDY3ExBj"
      },
      "id": "wuLRiDY3ExBj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "21755d9e",
      "metadata": {
        "id": "21755d9e"
      },
      "source": [
        "## 绘制对比图像"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91dfdc21",
      "metadata": {
        "id": "91dfdc21"
      },
      "outputs": [],
      "source": [
        "def Draw_compare_figure(orginal_X,orginal_Y,filtered_X,filtered_y):\n",
        "    # Plot the noisy and denoised trajectories\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(orginal_X, orginal_Y, 'kx', markersize=3, label='Noisy trajectory')\n",
        "    plt.plot(filtered_X,filtered_y, 'b-',markersize=0.5, label='Denoised trajectory')\n",
        "    plt.legend(loc='best')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#执行降噪函数"
      ],
      "metadata": {
        "id": "XBDqdnngxrZJ"
      },
      "id": "XBDqdnngxrZJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 小波变换对轨迹进行降噪"
      ],
      "metadata": {
        "id": "D1xkFgfexE0R"
      },
      "id": "D1xkFgfexE0R"
    },
    {
      "cell_type": "code",
      "source": [
        "pathdata = r\"/content/drive/My Drive/data_set/Mask_RCNN//Mask_orginal.csv\"\n",
        "YOLOV7_pathdata = r\"/content/drive/My Drive/data_set/Yolodata/S_raw.csv\"\n",
        "mask_data = pd.read_csv(YOLOV7_pathdata)\n",
        "df = pd.DataFrame(mask_data)\n",
        "vehid = pd.unique(mask_data.Object_id)\n",
        "Wavelet_traj = mask_data[['Object_id','frame_id','Vehicle_type']]\n",
        "Wt = pd.DataFrame(Wavelet_traj)\n",
        "Wt['World_x']=''\n",
        "Wt['World_y']=''\n",
        "Wt['speed_x']=''\n",
        "Wt['speed_y']=''\n",
        "Wt['acc_x']=''\n",
        "Wt['acc_y']=''\n",
        "Wt['Jerk_x']=''\n",
        "Wt['Jerk_y']=''\n",
        "# \"World_y\",\"speed_x\",\"speed_y\",\"acc_x\",\"acc_y\",\"speed_xy\",\"acc_xy\",\n",
        "for id in vehid:\n",
        "    B =[]\n",
        "    V =[]\n",
        "    ACC = []\n",
        "    Jerk=[]\n",
        "    traj_data = mask_data[mask_data.Object_id==id]\n",
        "    Frame_id = pd.unique(traj_data.frame_id)\n",
        "    TRAJ_world_x = traj_data['World_x']\n",
        "    TRAJ_world_y = traj_data['World_y']\n",
        "    min_row = traj_data.loc[traj_data['frame_id']== min(Frame_id),].index[0]\n",
        "    max_row = traj_data.loc[traj_data['frame_id']== max(Frame_id),].index[0]\n",
        "    A = df.iloc[min_row:max_row+1,[3,4]] \n",
        "    W_X = pd.array(wavelet_noising(TRAJ_world_x))\n",
        "    B.append(W_X)\n",
        "    W_Y = pd.array(wavelet_noising(TRAJ_world_y))\n",
        "    B.append(W_Y)\n",
        "    BB = pd.DataFrame(B)\n",
        "    BBB = np.transpose(BB)\n",
        "    if A.shape[0]<BBB.shape[0]:\n",
        "        CB = BBB.iloc[0:A.shape[0],[0,1]]\n",
        "    else:\n",
        "        CB = BBB.iloc[0:len(BBB),[0,1]]\n",
        "    Wt.iloc[min_row:max_row+1,[3,4]] = CB\n",
        "    # 计算速度     \n",
        "    WTraj = Wt.iloc[min_row:max_row+1,0:5]\n",
        "    speed_x,speed_y = Velocity(WTraj)\n",
        "    V.append(speed_x)\n",
        "    V.append(speed_y)\n",
        "    VV = pd.DataFrame(V)\n",
        "    VVV = np.transpose(VV)\n",
        "    Wt.iloc[min_row:max_row+1,[5,6]] = VVV\n",
        "    # 计算加速度\n",
        "    WWTraj = Wt.iloc[min_row:max_row+1,0:7]\n",
        "    acc_x,acc_y = Accelection(WWTraj)\n",
        "    ACC.append(acc_x)\n",
        "    ACC.append(acc_y)\n",
        "    ACCC = pd.DataFrame(ACC)\n",
        "    AACCC = np.transpose(ACCC)\n",
        "    Wt.iloc[min_row:max_row+1,[7,8]] = AACCC\n",
        "    # 计算加加速度\n",
        "Wt.to_csv('/content/drive/My Drive/data_set/Wavelet_denoise/Yolov7_denoise.csv')"
      ],
      "metadata": {
        "id": "B5Qu6IFwpgjg"
      },
      "id": "B5Qu6IFwpgjg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hampel Filter 降噪"
      ],
      "metadata": {
        "id": "rsuSTtGUxMPV"
      },
      "id": "rsuSTtGUxMPV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c51c0e2",
      "metadata": {
        "id": "9c51c0e2"
      },
      "outputs": [],
      "source": [
        "pathdata = r\"/content/drive/My Drive/data_set/Mask_RCNN//Mask_orginal.csv\"\n",
        "YOLOV7_pathdata = r\"/content/drive/My Drive/data_set/Yolodata/S_raw.csv\"\n",
        "mask_data = pd.read_csv(YOLOV7_pathdata)\n",
        "df = pd.DataFrame(mask_data)\n",
        "vehid = pd.unique(mask_data.Object_id)\n",
        "Wavelet_traj = mask_data[['Object_id','frame_id','Vehicle_type']]\n",
        "Wt = pd.DataFrame(Wavelet_traj)\n",
        "Wt['World_x']=''\n",
        "Wt['World_y']=''\n",
        "Wt['speed_x']=''\n",
        "Wt['speed_y']=''\n",
        "Wt['acc_x']=''\n",
        "Wt['acc_y']=''\n",
        "Wt['Jerk_x']=''\n",
        "Wt['Jerk_y']=''\n",
        "# \"World_y\",\"speed_x\",\"speed_y\",\"acc_x\",\"acc_y\",\"speed_xy\",\"acc_xy\",\n",
        "for id in vehid:\n",
        "    B =[]\n",
        "    V =[]\n",
        "    ACC = []\n",
        "    Jerk=[]\n",
        "    traj_data = mask_data[mask_data.Object_id==id]\n",
        "    Frame_id = pd.unique(traj_data.frame_id)\n",
        "    TRAJ_world_x = traj_data['World_x'].values\n",
        "    TRAJ_world_y = traj_data['World_y'].values\n",
        "    min_row = traj_data.loc[traj_data['frame_id']== min(Frame_id),].index[0]\n",
        "    max_row = traj_data.loc[traj_data['frame_id']== max(Frame_id),].index[0]\n",
        "    A = df.iloc[min_row:max_row+1,[3,4]] \n",
        "    W_X = pd.array(hampel_filter(TRAJ_world_x,10,3))\n",
        "    B.append(W_X)\n",
        "    W_Y = pd.array(hampel_filter(TRAJ_world_y,10,3))\n",
        "    B.append(W_Y)\n",
        "    BB = pd.DataFrame(B)\n",
        "    BBB = np.transpose(BB)\n",
        "    if A.shape[0]<BBB.shape[0]:\n",
        "        CB = BBB.iloc[0:A.shape[0],[0,1]]\n",
        "    else:\n",
        "        CB = BBB.iloc[0:len(BBB),[0,1]]\n",
        "    Wt.iloc[min_row:max_row+1,[3,4]] = CB\n",
        "    # 计算速度     \n",
        "    WTraj = Wt.iloc[min_row:max_row+1,0:5]\n",
        "    speed_x,speed_y = Velocity(WTraj)\n",
        "    V.append(speed_x)\n",
        "    V.append(speed_y)\n",
        "    VV = pd.DataFrame(V)\n",
        "    VVV = np.transpose(VV)\n",
        "    Wt.iloc[min_row:max_row+1,[5,6]] = VVV\n",
        "    # 计算加速度\n",
        "    WWTraj = Wt.iloc[min_row:max_row+1,0:7]\n",
        "    acc_x,acc_y = Accelection(WWTraj)\n",
        "    ACC.append(acc_x)\n",
        "    ACC.append(acc_y)\n",
        "    ACCC = pd.DataFrame(ACC)\n",
        "    AACCC = np.transpose(ACCC)\n",
        "    Wt.iloc[min_row:max_row+1,[7,8]] = AACCC\n",
        "    # 计算加加速度\n",
        "Wt.to_csv('/content/drive/My Drive/data_set/Hampel_denoise/Yolov7_denoise.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Penalized 轨迹降噪"
      ],
      "metadata": {
        "id": "NZutmivgzKIE"
      },
      "id": "NZutmivgzKIE"
    },
    {
      "cell_type": "code",
      "source": [
        "pathdata = r\"/content/drive/My Drive/data_set/Mask_RCNN//Mask_orginal.csv\"\n",
        "YOLOV7_pathdata = r\"/content/drive/My Drive/data_set/Yolodata/S_raw.csv\"\n",
        "mask_data = pd.read_csv(YOLOV7_pathdata)\n",
        "df = pd.DataFrame(mask_data)\n",
        "vehid = pd.unique(mask_data.Object_id)\n",
        "Wavelet_traj = mask_data[['Object_id','frame_id','Vehicle_type']]\n",
        "Wt = pd.DataFrame(Wavelet_traj)\n",
        "Wt['World_x']=''\n",
        "Wt['World_y']=''\n",
        "Wt['speed_x']=''\n",
        "Wt['speed_y']=''\n",
        "Wt['acc_x']=''\n",
        "Wt['acc_y']=''\n",
        "Wt['Jerk_x']=''\n",
        "Wt['Jerk_y']=''\n",
        "# \"World_y\",\"speed_x\",\"speed_y\",\"acc_x\",\"acc_y\",\"speed_xy\",\"acc_xy\",\n",
        "for id in vehid:\n",
        "    B =[]\n",
        "    V =[]\n",
        "    ACC = []\n",
        "    Jerk=[]\n",
        "    traj_data = mask_data[mask_data.Object_id==id]\n",
        "    Frame_id = pd.unique(traj_data.frame_id)\n",
        "    TRAJ_world_x = traj_data['World_x'].values\n",
        "    TRAJ_world_y = traj_data['World_y'].values\n",
        "    min_row = traj_data.loc[traj_data['frame_id']== min(Frame_id),].index[0]\n",
        "    max_row = traj_data.loc[traj_data['frame_id']== max(Frame_id),].index[0]\n",
        "    A = df.iloc[min_row:max_row+1,[3,4]] \n",
        "    W_X = pd.array(Denoise_penalized(TRAJ_world_x,1))\n",
        "    B.append(W_X)\n",
        "    W_Y = pd.array(Denoise_penalized(TRAJ_world_y,1))\n",
        "    B.append(W_Y)\n",
        "    BB = pd.DataFrame(B)\n",
        "    BBB = np.transpose(BB)\n",
        "    if A.shape[0]<BBB.shape[0]:\n",
        "        CB = BBB.iloc[0:A.shape[0],[0,1]]\n",
        "    else:\n",
        "        CB = BBB.iloc[0:len(BBB),[0,1]]\n",
        "    Wt.iloc[min_row:max_row+1,[3,4]] = CB\n",
        "    # 计算速度     \n",
        "    WTraj = Wt.iloc[min_row:max_row+1,0:5]\n",
        "    speed_x,speed_y = Velocity(WTraj)\n",
        "    V.append(speed_x)\n",
        "    V.append(speed_y)\n",
        "    VV = pd.DataFrame(V)\n",
        "    VVV = np.transpose(VV)\n",
        "    Wt.iloc[min_row:max_row+1,[5,6]] = VVV\n",
        "    # 计算加速度\n",
        "    WWTraj = Wt.iloc[min_row:max_row+1,0:7]\n",
        "    acc_x,acc_y = Accelection(WWTraj)\n",
        "    ACC.append(acc_x)\n",
        "    ACC.append(acc_y)\n",
        "    ACCC = pd.DataFrame(ACC)\n",
        "    AACCC = np.transpose(ACCC)\n",
        "    Wt.iloc[min_row:max_row+1,[7,8]] = AACCC\n",
        "    # 计算加加速度\n",
        "Wt.to_csv('/content/drive/My Drive/data_set/Penalized_denoise/Yolov7_denoise.csv')"
      ],
      "metadata": {
        "id": "73WKaRUEhhEs"
      },
      "id": "73WKaRUEhhEs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f181bddf",
      "metadata": {
        "id": "f181bddf"
      },
      "source": [
        "#### 利用Laplace Filter对轨迹进行降噪处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8324f6",
      "metadata": {
        "id": "1d8324f6"
      },
      "outputs": [],
      "source": [
        "pathdata = r\"/content/drive/My Drive/data_set/Mask_RCNN//Mask_orginal.csv\"\n",
        "YOLOV7_pathdata = r\"/content/drive/My Drive/data_set/Yolodata/S_raw.csv\"\n",
        "mask_data = pd.read_csv(YOLOV7_pathdata)\n",
        "df = pd.DataFrame(mask_data)\n",
        "vehid = pd.unique(mask_data.Object_id)\n",
        "Wavelet_traj = mask_data[['Object_id','frame_id','Vehicle_type']]\n",
        "Wt = pd.DataFrame(Wavelet_traj)\n",
        "Wt['World_x']=''\n",
        "Wt['World_y']=''\n",
        "Wt['speed_x']=''\n",
        "Wt['speed_y']=''\n",
        "Wt['acc_x']=''\n",
        "Wt['acc_y']=''\n",
        "Wt['Jerk_x']=''\n",
        "Wt['Jerk_y']=''\n",
        "# \"World_y\",\"speed_x\",\"speed_y\",\"acc_x\",\"acc_y\",\"speed_xy\",\"acc_xy\",\n",
        "for id in vehid:\n",
        "    B =[]\n",
        "    V =[]\n",
        "    ACC = []\n",
        "    Jerk=[]\n",
        "    traj_data = mask_data[mask_data.Object_id==id]\n",
        "    Frame_id = pd.unique(traj_data.frame_id)\n",
        "    TRAJ_world_x = traj_data['World_x'].values\n",
        "    TRAJ_world_y = traj_data['World_y'].values\n",
        "    min_row = traj_data.loc[traj_data['frame_id']== min(Frame_id),].index[0]\n",
        "    max_row = traj_data.loc[traj_data['frame_id']== max(Frame_id),].index[0]\n",
        "    A = df.iloc[min_row:max_row+1,[3,4]] \n",
        "\n",
        "    W_X = pd.array(Laplacian_denoise(TRAJ_world_x))\n",
        "    B.append(W_X)\n",
        "    W_Y = pd.array(Laplacian_denoise(TRAJ_world_y))\n",
        "    B.append(W_Y)\n",
        "    BB = pd.DataFrame(B)\n",
        "    BBB = np.transpose(BB)\n",
        "    if A.shape[0]<BBB.shape[0]:\n",
        "        CB = BBB.iloc[0:A.shape[0],[0,1]]\n",
        "    else:\n",
        "        CB = BBB.iloc[0:len(BBB),[0,1]]\n",
        "    Wt.iloc[min_row:max_row+1,[3,4]] = CB\n",
        "    # 计算速度     \n",
        "    WTraj = Wt.iloc[min_row:max_row+1,0:5]\n",
        "    speed_x,speed_y = Velocity(WTraj)\n",
        "    V.append(speed_x)\n",
        "    V.append(speed_y)\n",
        "    VV = pd.DataFrame(V)\n",
        "    VVV = np.transpose(VV)\n",
        "    Wt.iloc[min_row:max_row+1,[5,6]] = VVV\n",
        "    # 计算加速度\n",
        "    WWTraj = Wt.iloc[min_row:max_row+1,0:7]\n",
        "    acc_x,acc_y = Accelection(WWTraj)\n",
        "    ACC.append(acc_x)\n",
        "    ACC.append(acc_y)\n",
        "    ACCC = pd.DataFrame(ACC)\n",
        "    AACCC = np.transpose(ACCC)\n",
        "    Wt.iloc[min_row:max_row+1,[7,8]] = AACCC\n",
        "    # 计算加加速度\n",
        "Wt.to_csv('/content/drive/My Drive/data_set/Laplace_denoise/Yolov7_denoise.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM_DENOISE"
      ],
      "metadata": {
        "id": "2vPYoDg0GOFv"
      },
      "id": "2vPYoDg0GOFv"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Reshape\n",
        "\n",
        "# Generate noisy vehicle trajectory data\n",
        "def LSTM_denoise(trajectory):\n",
        "    n_samples = len(trajectory)\n",
        "    y = trajectory\n",
        "    t = 20\n",
        "    # Create input/output pairs for training the LSTM\n",
        "    window_size = t\n",
        "    X_train = np.zeros((n_samples-window_size, window_size, 1))\n",
        "    y_train = np.zeros((n_samples-window_size, 1))\n",
        "    for i in range(n_samples-window_size):\n",
        "        X_train[i, :, 0] = y[i:i+window_size]\n",
        "        y_train[i, 0] = y[i+window_size]\n",
        "\n",
        "    # Create an LSTM model for denoising the trajectory data\n",
        "    model = Sequential([\n",
        "        LSTM(32, input_shape=(window_size, 1), return_sequences=True),\n",
        "        LSTM(16, return_sequences=True),\n",
        "        LSTM(8),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile the model and train it on the noisy trajectory data\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train, y_train, epochs=1000)\n",
        "\n",
        "    # Use the trained model to denoise the trajectory data\n",
        "    y_denoised = model.predict(X_train)\n",
        "\n",
        "    list_a_W_X = [item for sublist in y_denoised for item in sublist]\n",
        "    list_A_W_X = list(list_a_W_X)\n",
        "    list_B_W_X = list(trajectory[0:t])\n",
        "    list_B_W_X.extend(list_A_W_X)\n",
        "    Lstm_denoise=np.array(list_B_W_X)\n",
        "    return Lstm_denoise"
      ],
      "metadata": {
        "id": "nAVk5CkRGVrL"
      },
      "id": "nAVk5CkRGVrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathdata = r\"/content/drive/My Drive/data_set/Mask_RCNN//Mask_orginal.csv\"\n",
        "YOLOV7_pathdata = r\"/content/drive/My Drive/data_set/Yolodata/S_raw.csv\"\n",
        "mask_data = pd.read_csv(YOLOV7_pathdata)\n",
        "df = pd.DataFrame(mask_data)\n",
        "vehid = pd.unique(mask_data.Object_id)\n",
        "Wavelet_traj = mask_data[['Object_id','frame_id','Vehicle_type']]\n",
        "Wt = pd.DataFrame(Wavelet_traj)\n",
        "Wt['World_x']=''\n",
        "Wt['World_y']=''\n",
        "Wt['speed_x']=''\n",
        "Wt['speed_y']=''\n",
        "Wt['acc_x']=''\n",
        "Wt['acc_y']=''\n",
        "Wt['Jerk_x']=''\n",
        "Wt['Jerk_y']=''\n",
        "# \"World_y\",\"speed_x\",\"speed_y\",\"acc_x\",\"acc_y\",\"speed_xy\",\"acc_xy\",\n",
        "for id in vehid:\n",
        "    print('----------------------------------------------------------')\n",
        "    B =[]\n",
        "    V =[]\n",
        "    ACC = []\n",
        "    Jerk=[]\n",
        "    traj_data = mask_data[mask_data.Object_id==id]\n",
        "    Frame_id = pd.unique(traj_data.frame_id)\n",
        "    TRAJ_world_x = traj_data['World_x'].values\n",
        "    TRAJ_world_y = traj_data['World_y'].values\n",
        "    min_row = traj_data.loc[traj_data['frame_id']== min(Frame_id),].index[0]\n",
        "    max_row = traj_data.loc[traj_data['frame_id']== max(Frame_id),].index[0]\n",
        "    A = df.iloc[min_row:max_row+1,[3,4]] \n",
        "    W_X = LSTM_denoise(TRAJ_world_x)\n",
        "    B.append(W_X)\n",
        "    W_Y = LSTM_denoise(TRAJ_world_y)\n",
        "    B.append(W_Y)\n",
        "    BB = pd.DataFrame(B)\n",
        "    BBB = np.transpose(BB)\n",
        "    if A.shape[0]<BBB.shape[0]:\n",
        "        CB = BBB.iloc[0:A.shape[0],[0,1]]\n",
        "    else:\n",
        "        CB = BBB.iloc[0:len(BBB),[0,1]]\n",
        "    Wt.iloc[min_row:max_row+1,[3,4]] = CB\n",
        "    # 计算速度     \n",
        "    WTraj = Wt.iloc[min_row:max_row+1,0:5]\n",
        "    speed_x,speed_y = Velocity(WTraj)\n",
        "    V.append(speed_x)\n",
        "    V.append(speed_y)\n",
        "    VV = pd.DataFrame(V)\n",
        "    VVV = np.transpose(VV)\n",
        "    Wt.iloc[min_row:max_row+1,[5,6]] = VVV\n",
        "    # 计算加速度\n",
        "    WWTraj = Wt.iloc[min_row:max_row+1,0:7]\n",
        "    acc_x,acc_y = Accelection(WWTraj)\n",
        "    ACC.append(acc_x)\n",
        "    ACC.append(acc_y)\n",
        "    ACCC = pd.DataFrame(ACC)\n",
        "    AACCC = np.transpose(ACCC)\n",
        "    Wt.iloc[min_row:max_row+1,[7,8]] = AACCC\n",
        "    # 计算加加速度\n",
        "Wt.to_csv('/content/drive/My Drive/data_set/LSTM_denoise/Yolov7_denoise.csv')"
      ],
      "metadata": {
        "id": "mTyjJ2aw4ntH"
      },
      "id": "mTyjJ2aw4ntH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vf5I326_1Gls"
      },
      "id": "Vf5I326_1Gls"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 卷积神经网络函数定义，CNN_denoise"
      ],
      "metadata": {
        "id": "_yQXApd3LSZX"
      },
      "id": "_yQXApd3LSZX"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Reshape\n",
        "\n",
        "def CNN_denoise(trajectory):\n",
        "    # Generate noisy vehicle trajectory data\n",
        "    n_samples = len(trajectory)\n",
        "    print(n_samples)\n",
        "    y = trajectory\n",
        "    # Create input/output pairs for training the CNN\n",
        "    window_size = 25\n",
        "    X_train = np.zeros((n_samples-window_size, window_size, 1))\n",
        "    y_train = np.zeros((n_samples-window_size, 1))\n",
        "    for i in range(n_samples-window_size):\n",
        "        X_train[i, :, 0] = y[i:i+window_size]\n",
        "        y_train[i, 0] = y[i+window_size]\n",
        "    # Create a CNN model for denoising the trajectory data\n",
        "    model = Sequential([\n",
        "        Conv1D(16, 3, activation='relu', input_shape=(window_size, 1)),\n",
        "        MaxPooling1D(2),\n",
        "        Conv1D(32, 3, activation='relu'),\n",
        "        MaxPooling1D(2),\n",
        "        Flatten(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    # Compile the model and train it on the noisy trajectory data\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train, y_train, epochs=100)\n",
        "    # Use the trained model to denoise the trajectory data\n",
        "    y_denoised = model.predict(X_train)\n",
        "    list_a_W_X = [item for sublist in y_denoised for item in sublist]\n",
        "    list_A_W_X = list(list_a_W_X)\n",
        "    list_B_W_X = list(trajectory[0:25])\n",
        "    list_B_W_X.extend(list_A_W_X)\n",
        "    a_denoise=np.array(list_B_W_X) \n",
        "    return a_denoise"
      ],
      "metadata": {
        "id": "NaulNwm1LdXA"
      },
      "id": "NaulNwm1LdXA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###卷积神经网络降噪-CNN_denoise"
      ],
      "metadata": {
        "id": "pZXE5IO4XUdr"
      },
      "id": "pZXE5IO4XUdr"
    },
    {
      "cell_type": "code",
      "source": [
        "pathdata = r\"/content/drive/My Drive/data_set/Mask_RCNN//Mask_orginal.csv\"\n",
        "YOLOV7_pathdata = r\"/content/drive/My Drive/data_set/Yolodata/S_raw.csv\"\n",
        "mask_data = pd.read_csv(YOLOV7_pathdata)\n",
        "df = pd.DataFrame(mask_data)\n",
        "vehid = pd.unique(mask_data.Object_id)\n",
        "Wavelet_traj = mask_data[['Object_id','frame_id','Vehicle_type']]\n",
        "Wt = pd.DataFrame(Wavelet_traj)\n",
        "Wt['World_x']=''\n",
        "Wt['World_y']=''\n",
        "Wt['speed_x']=''\n",
        "Wt['speed_y']=''\n",
        "Wt['acc_x']=''\n",
        "Wt['acc_y']=''\n",
        "Wt['Jerk_x']=''\n",
        "Wt['Jerk_y']=''\n",
        "# \"World_y\",\"speed_x\",\"speed_y\",\"acc_x\",\"acc_y\",\"speed_xy\",\"acc_xy\",\n",
        "for id in vehid:\n",
        "    print('----------------------------------------------------------')\n",
        "    B =[]\n",
        "    V =[]\n",
        "    ACC = []\n",
        "    Jerk=[]\n",
        "    traj_data = mask_data[mask_data.Object_id==id]\n",
        "    Frame_id = pd.unique(traj_data.frame_id)\n",
        "    TRAJ_world_x = traj_data['World_x'].values\n",
        "    TRAJ_world_y = traj_data['World_y'].values\n",
        "    min_row = traj_data.loc[traj_data['frame_id']== min(Frame_id),].index[0]\n",
        "    max_row = traj_data.loc[traj_data['frame_id']== max(Frame_id),].index[0]\n",
        "    A = df.iloc[min_row:max_row+1,[3,4]] \n",
        "    W_X = CNN_denoise(TRAJ_world_x)\n",
        "    B.append(W_X)\n",
        "    W_Y = CNN_denoise(TRAJ_world_y)\n",
        "    B.append(W_Y)\n",
        "    BB = pd.DataFrame(B)\n",
        "    BBB = np.transpose(BB)\n",
        "    if A.shape[0]<BBB.shape[0]:\n",
        "        CB = BBB.iloc[0:A.shape[0],[0,1]]\n",
        "    else:\n",
        "        CB = BBB.iloc[0:len(BBB),[0,1]]\n",
        "    Wt.iloc[min_row:max_row+1,[3,4]] = CB\n",
        "    # 计算速度     \n",
        "    WTraj = Wt.iloc[min_row:max_row+1,0:5]\n",
        "    speed_x,speed_y = Velocity(WTraj)\n",
        "    V.append(speed_x)\n",
        "    V.append(speed_y)\n",
        "    VV = pd.DataFrame(V)\n",
        "    VVV = np.transpose(VV)\n",
        "    Wt.iloc[min_row:max_row+1,[5,6]] = VVV\n",
        "    # 计算加速度\n",
        "    WWTraj = Wt.iloc[min_row:max_row+1,0:7]\n",
        "    acc_x,acc_y = Accelection(WWTraj)\n",
        "    ACC.append(acc_x)\n",
        "    ACC.append(acc_y)\n",
        "    ACCC = pd.DataFrame(ACC)\n",
        "    AACCC = np.transpose(ACCC)\n",
        "    Wt.iloc[min_row:max_row+1,[7,8]] = AACCC\n",
        "    # 计算加加速度\n",
        "Wt.to_csv('/content/drive/My Drive/data_set/CNN_denoise/Yolov7_denoise.csv')"
      ],
      "metadata": {
        "id": "bqPzgA9iSaNp"
      },
      "id": "bqPzgA9iSaNp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 基于自编码生成模型的降噪"
      ],
      "metadata": {
        "id": "e8Cr5WVH6LVr"
      },
      "id": "e8Cr5WVH6LVr"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kW5GfMu46TX0"
      },
      "id": "kW5GfMu46TX0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 基于自编码处理轨迹数据集"
      ],
      "metadata": {
        "id": "-rs63qA366gp"
      },
      "id": "-rs63qA366gp"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Qw0YKEG7CQO"
      },
      "id": "7Qw0YKEG7CQO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 基于生成模型的轨迹降噪函数定义"
      ],
      "metadata": {
        "id": "EdgmfPIn7DA1"
      },
      "id": "EdgmfPIn7DA1"
    },
    {
      "cell_type": "code",
      "source": [
        "# 列出你云盘里有啥\n",
        "!ls \"/content/drive/My Drive/data_set/Ground_truth\""
      ],
      "metadata": {
        "id": "cAWPxcVL7MkI"
      },
      "id": "cAWPxcVL7MkI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "基于自编码轨迹数据处理"
      ],
      "metadata": {
        "id": "fAMsNeRl7Lt8"
      },
      "id": "fAMsNeRl7Lt8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 对比不同处理结果，计算对比指标"
      ],
      "metadata": {
        "id": "ah55nGUchgXK"
      },
      "id": "ah55nGUchgXK"
    },
    {
      "cell_type": "code",
      "source": [
        "## 导入对应id数据\n",
        "# 导入高低密度下的车辆对应id列表\n",
        "# 导入ground turth dataset\n",
        "pathG_Y_ID = r'/content/drive/My Drive/data_set/ID_MATCH/YOLOV7_GROUND/high.csv'\n",
        "G_Y_ID_high = pd.read_csv(pathG_Y_ID)\n",
        "gound_yolo_ID_high = pd.DataFrame(G_Y_ID_high) #swatch the dict to dataframe\n",
        "# 导入低密度下的车辆对应id列表\n",
        "# 导入ground turth dataset\n",
        "pathG_Y_ID = r'/content/drive/My Drive/data_set/ID_MATCH/YOLOV7_GROUND/low.csv'\n",
        "G_Y_ID_low = pd.read_csv(pathG_Y_ID)\n",
        "gound_yolo_ID_low = pd.DataFrame(G_Y_ID_low) #swatch the dict to dataframe\n",
        "\n",
        "## 导入ground_truth and Mask_RCNN对应id数据\n",
        "# 导入低密度下的车辆对应id的列表\n",
        "pathG_M_ID = r'/content/drive/My Drive/data_set/ID_MATCH/Mask_Ground/low.csv'\n",
        "G_M_ID_LOW = pd.read_csv(pathG_M_ID)\n",
        "gound_mask_ID_LOW = pd.DataFrame(G_M_ID_LOW) #swatch the dict to dataframe\n",
        "# 导入高密度下的车辆对应id列表\n",
        "# 导入ground turth dataset\n",
        "pathG_m_HID = r'/content/drive/My Drive/data_set/ID_MATCH/Mask_Ground/high.csv'\n",
        "G_M_ID_HIHG = pd.read_csv(pathG_m_HID)\n",
        "gound_mask_ID_high = pd.DataFrame(G_M_ID_HIHG) #swatch the dict to dataframe\n",
        "\n",
        "Ground_truth_traj = r'/content/drive/My Drive/data_set/Ground_truth/Geroge_ST_trajectory.csv'\n",
        "Ground_truth = pd.read_csv(Ground_truth_traj)\n",
        "G_TRAJ_T = pd.DataFrame(Ground_truth)"
      ],
      "metadata": {
        "id": "cEGqVl05hk-J"
      },
      "id": "cEGqVl05hk-J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the indicator "
      ],
      "metadata": {
        "id": "cYIOLYvjkzXt"
      },
      "id": "cYIOLYvjkzXt"
    },
    {
      "cell_type": "code",
      "source": [
        "# 对齐轨迹点的函数\n",
        "def Alignment(Groundtruthebike,Comparetraj):\n",
        "#     traj_ground0 = Groundtruthebike[Groundtruthebike.Object_id ==ground_id]\n",
        "#     print(Groundtruthebike,Comparetraj)\n",
        "    min_Y_G = min(pd.array(Groundtruthebike.World_y))\n",
        "    max_Y_G = max(pd.array(Groundtruthebike.World_y))\n",
        "    # CV输出的轨迹数据\n",
        "#     Comparetraj0 = Comparetraj[Comparetraj.Object_id == yolo_id]\n",
        "    min_Y_C = min(pd.array(Comparetraj.World_y))\n",
        "    max_Y_C = max(pd.array(Comparetraj.World_y))\n",
        "    bottom = max(min_Y_G,min_Y_C)\n",
        "    top = min(max_Y_G,max_Y_C)\n",
        "    #让轨迹点对齐\n",
        "    # 需要对齐时间帧\n",
        "#     print(\"------------------------1--------------------------\")\n",
        "    traj_ground1 = Groundtruthebike[Groundtruthebike.World_y>=bottom]\n",
        "    traj_ground = traj_ground1[traj_ground1.World_y<=top] \n",
        "#     traj_ground['G_frameid'] = range(1:traj_ground.shape[0])\n",
        "    Comparetraj1 = Comparetraj[Comparetraj.World_y>=bottom]\n",
        "    traj_compare = Comparetraj1[Comparetraj1.World_y<=top] \n",
        "#     print(\"-----------------------2---------------------------\")\n",
        "    #设置时间帧对齐的方法\n",
        "    if traj_ground.shape[0]>=traj_compare.shape[0]:\n",
        "        frame_ids =  traj_ground.frame_id[0:traj_compare.shape[0]]\n",
        "#         print(\"----------------------------1------------------------\")\n",
        "        traj_compare['G_frameid'] = pd.array(frame_ids)\n",
        "#         print(traj_compare.shape[0])        \n",
        "    else:\n",
        "        frame_ids = traj_ground.frame_id\n",
        "#         print(\"----------------------------2------------------------\")\n",
        "#         print(traj_ground.shape[0])\n",
        "        traj_compare = traj_compare[0:traj_ground.shape[0]]\n",
        "#         print(traj_compare.shape[0])\n",
        "        traj_compare['G_frameid'] = pd.array(frame_ids)   \n",
        "#         print(traj_compare)     \n",
        "    return traj_ground,traj_compare"
      ],
      "metadata": {
        "id": "R7AB2i4ClIyY"
      },
      "id": "R7AB2i4ClIyY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算轨迹中的速度/加速度/加加速度\n",
        "def XY(groundtraj,caompartraj):\n",
        "    g_World_x = np.array(groundtraj['World_x'].astype(float))\n",
        "    g_World_y = np.array(groundtraj['World_y'].astype(float))\n",
        "    com_World_x = np.array(caompartraj['World_x'].astype(float))\n",
        "    com_World_y = np.array(caompartraj['World_y'].astype(float))    \n",
        "    return g_World_x,g_World_y,com_World_x,com_World_y\n",
        "\n",
        "def Velocity(trajdata):\n",
        "    \"\"\"recalculate the velocity, the velocity contain the X_velocity and Y-velocity and the speed\"\"\"\n",
        "    \"\"\"定义初速度为0\"\"\"\n",
        "    len_x = trajdata.shape[0]\n",
        "    wordld_x = np.array(trajdata.World_x)\n",
        "    wordld_y = np.array(trajdata.World_y)\n",
        "    velocity_x = (wordld_x[1:len_x]-wordld_x[0:len_x-1])/0.04\n",
        "    velocity_y = (wordld_y[1:len_x]-wordld_y[0:len_x-1])/0.04\n",
        "    velocity_x = np.insert(velocity_x,0,0)\n",
        "    velocity_y = np.insert(velocity_y,0,0)\n",
        "    return velocity_x,velocity_y\n",
        "\n",
        "def Accelection(trajdata):\n",
        "    \"\"\"定结束时刻的加速度为0\"\"\"\n",
        "    len_x = trajdata.shape[0]\n",
        "    velocity_x = np.array(trajdata.speed_x)\n",
        "    velocity_y = np.array(trajdata.speed_y)\n",
        "    accelection_x = (velocity_x[1:len_x]-velocity_x[0:len_x-1])/0.04\n",
        "    accelection_y = (velocity_y[1:len_x]-velocity_y[0:len_x-1])/0.04\n",
        "    accelection_x = np.insert(accelection_x,0,0)\n",
        "    accelection_y = np.insert(accelection_y,0,0)"
      ],
      "metadata": {
        "id": "L_y6n3HWDPqa"
      },
      "id": "L_y6n3HWDPqa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Samelen(GTRAJ,COMTRAJ): \n",
        "    frame_len = min(len(GTRAJ),len(COMTRAJ))\n",
        "    traj_ground2 = GTRAJ.iloc[:frame_len]\n",
        "    traj_compare2 = COMTRAJ[:frame_len]\n",
        "    return traj_ground2,traj_compare2"
      ],
      "metadata": {
        "id": "a_hJRhzGzsJ_"
      },
      "id": "a_hJRhzGzsJ_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 二维坐标显示\n",
        "def Trajmap(traj_ground,traj_compare):\n",
        "    # Trajdatas对比轨迹，traj_compare是真实轨迹\n",
        "    fig1 = plt.figure(figsize=(8,4))   \n",
        "    g_World_x,g_World_y,com_World_x,com_World_y = XY(traj_ground,traj_compare) \n",
        "    plt.plot(g_World_x,g_World_y,color=\"blue\",label='Ground trajectory')\n",
        "    plt.plot(com_World_x,com_World_y,color=\"red\",label='Yolov7 trajectory')\n",
        "    plt.legend()\n",
        "    plt.legend(loc=\"upper right\",fontsize=12)   #设置图例字体大小\n",
        "    plt.xlabel('X(m))',fontsize=16)     #设置x轴名称\n",
        "    plt.legend(loc=\"upper right\",fontsize=16)   #设置图例字体大小\n",
        "    # 坐标尺\n",
        "    plt.xticks(fontsize=12,rotation=0)\n",
        "    plt.yticks(fontsize=12)\n",
        "    # 坐标名称\n",
        "    plt.rcParams.update({\"font.size\":16})\n",
        "    plt.title(\"The trajectory\",fontsize=16)\n",
        "    plt.ylabel('Y(m)',fontsize=16)     #设置y轴名称\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    return fig1  "
      ],
      "metadata": {
        "id": "mcvaQ1nK8zxu"
      },
      "id": "mcvaQ1nK8zxu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSE\n",
        "def Index_rmse(traj_ground,traj_compare):\n",
        "    #比较真实轨迹与CV输出轨迹以及算法输出轨迹之间的差异\n",
        "    #选择相同的起点坐标，后面选取相同长度的轨迹点，比较两个数据集长度\n",
        "#     print(traj_ground,traj_compare)\n",
        "    ground_truth,Comparetraj = Samelen(traj_ground,traj_compare)\n",
        "    len_ground_truth = ground_truth.shape[0]\n",
        "    len_Compare_traj = Comparetraj.shape[0]     \n",
        "    num_traj_id = min(len_ground_truth,len_Compare_traj)    \n",
        "    Ground_traj = ground_truth[0:num_traj_id]    \n",
        "    Compar_traj = Comparetraj[0:num_traj_id]    \n",
        "    Ground_traj_world_x = np.array(Ground_traj.World_x)\n",
        "    Ground_traj_world_y = np.array(Ground_traj.World_y)\n",
        "    Compar_traj_world_x = np.array(Compar_traj.World_x)\n",
        "    Compar_traj_world_y = np.array(Compar_traj.World_y)\n",
        "    #计算真实轨迹点的差异\n",
        "    deltx = np.array(Ground_traj_world_x - Compar_traj_world_x)\n",
        "    delty = np.array(Ground_traj_world_y - Compar_traj_world_y)\n",
        "    RMSE = math.sqrt((np.sum(deltx*deltx) + np.sum(deltx*deltx))/num_traj_id)\n",
        "    return RMSE"
      ],
      "metadata": {
        "id": "AsoudiPzzywl"
      },
      "id": "AsoudiPzzywl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算信噪比\n",
        "def SNR(traj_ground,traj_compare):\n",
        "    # 计算信号与噪声的比值\n",
        "    #比较真实轨迹与CV输出轨迹以及算法输出轨迹之间的差异\n",
        "    #选择相同的起点坐标，后面选取相同长度的轨迹点，比较两个数据集长度\n",
        "    # print(traj_ground,traj_compare)\n",
        "    ground_truth,Comparetraj = Samelen(traj_ground,traj_compare)\n",
        "    len_ground_truth = ground_truth.shape[0]\n",
        "    len_Compare_traj = Comparetraj.shape[0]     \n",
        "    num_traj_id = min(len_ground_truth,len_Compare_traj)    \n",
        "    Ground_traj = ground_truth[0:num_traj_id]    \n",
        "    Compar_traj = Comparetraj[0:num_traj_id]    \n",
        "    Ground_traj_world_x = np.array(Ground_traj.World_x)\n",
        "    Ground_traj_world_y = np.array(Ground_traj.World_y)\n",
        "    Compar_traj_world_x = np.array(Compar_traj.World_x)\n",
        "    Compar_traj_world_y = np.array(Compar_traj.World_y)\n",
        "    #计算轨迹噪声    \n",
        "    deltx = np.array(Ground_traj_world_x - Compar_traj_world_x)\n",
        "    delty = np.array(Ground_traj_world_y - Compar_traj_world_y)\n",
        "    Noise = (np.sum(deltx*deltx) + np.sum(deltx*deltx))\n",
        "    # 计算轨迹信号\n",
        "    SIGNAL = np.sum(Compar_traj_world_x*Compar_traj_world_x+Compar_traj_world_y*Compar_traj_world_y)\n",
        "    SNRR = 10*math.log(SIGNAL/Noise)\n",
        "    return SNRR"
      ],
      "metadata": {
        "id": "z3CPla9R0B3R"
      },
      "id": "z3CPla9R0B3R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算R方\n",
        "def RR(traj_ground,traj_compare):\n",
        "    #计算轨迹的相似性\n",
        "    #比较真实轨迹与CV输出轨迹以及算法输出轨迹之间的差异\n",
        "    #选择相同的起点坐标，后面选取相同长度的轨迹点，比较两个数据集长度\n",
        "    # print(traj_ground,traj_compare)\n",
        "    ground_truth,Comparetraj = Samelen(traj_ground,traj_compare)\n",
        "    len_ground_truth = ground_truth.shape[0]\n",
        "    len_Compare_traj = Comparetraj.shape[0]     \n",
        "    num_traj_id = min(len_ground_truth,len_Compare_traj)    \n",
        "    Ground_traj = ground_truth[0:num_traj_id]    \n",
        "    Compar_traj = Comparetraj[0:num_traj_id]    \n",
        "    Ground_traj_world_x = np.array(Ground_traj.World_x)\n",
        "    Ground_traj_world_y = np.array(Ground_traj.World_y)\n",
        "    Compar_traj_world_x = np.array(Compar_traj.World_x)\n",
        "    Compar_traj_world_y = np.array(Compar_traj.World_y)\n",
        "    \n",
        "    #将坐标转换为距离起点的距离\n",
        "    DIS_G =  pd.DataFrame(Ground_traj_world_x*Ground_traj_world_x+Ground_traj_world_y*Ground_traj_world_y)\n",
        "    Dis_Ground_traj_world = DIS_G.apply(np.sqrt)\n",
        "    DIS_C =  pd.DataFrame(Compar_traj_world_x*Compar_traj_world_x+Compar_traj_world_y*Compar_traj_world_y)\n",
        "    Dis_Compar_traj_world = DIS_C.apply(np.sqrt)\n",
        "    \n",
        "    M_DIS_G = np.mean(Dis_Ground_traj_world)\n",
        "    M_DIS_C = np.mean(Dis_Compar_traj_world)\n",
        "    G_delta = Dis_Ground_traj_world-M_DIS_G\n",
        "    C_delta = Dis_Compar_traj_world- M_DIS_C\n",
        "    up_1 = np.array(G_delta)\n",
        "    up_2 = np.array(C_delta)\n",
        "    UP_total = np.multiply(up_1,up_2)\n",
        "    UP_total_value = np.sum(UP_total)\n",
        "    \n",
        "    \n",
        "    Botm_1 = np.sum(G_delta*G_delta)\n",
        "#     Botm_2 = np.sum(C_delta*C_delta)\n",
        "    Botm_2 = 1\n",
        "    Botm_total_value = np.multiply(Botm_1,Botm_2)\n",
        "    \n",
        "    print(\"-----------------\")\n",
        "    Rr = (UP_total_value/Botm_total_value)\n",
        "    print(Rr)\n",
        "    return Rr   "
      ],
      "metadata": {
        "id": "MEgFJUby0C7B"
      },
      "id": "MEgFJUby0C7B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from astropy.units import Ybarn\n",
        "import math\n",
        " \n",
        "def computeCorrelation(X, Y):\n",
        "    xBar = np.mean(X)\n",
        "    yBar = np.mean(Y)\n",
        "    SSR = 0\n",
        "    varX = 0\n",
        "    varY = 0\n",
        "    for i in range(0 , len(X)):\n",
        "        diffXXBar = X[i] - xBar\n",
        "        diffYYBar = Y[i] - yBar\n",
        "        SSR += (diffXXBar * diffYYBar)\n",
        "        varX +=  diffXXBar**2\n",
        "        varY += diffYYBar**2\n",
        "    \n",
        "    SST = math.sqrt(varX * varY)\n",
        "    if SST==0:\n",
        "        SST =1\n",
        "    return SSR / SST\n",
        " \n",
        "def RRR(traj_ground,traj_compare): \n",
        "    ground_truth,Comparetraj = Samelen(traj_ground,traj_compare)\n",
        "    len_ground_truth = ground_truth.shape[0]\n",
        "    len_Compare_traj = Comparetraj.shape[0]     \n",
        "    num_traj_id = min(len_ground_truth,len_Compare_traj)    \n",
        "    Ground_traj = ground_truth[0:num_traj_id]    \n",
        "    Compar_traj = Comparetraj[0:num_traj_id]    \n",
        "    Ground_traj_world_x = np.array(Ground_traj.World_x)\n",
        "    Ground_traj_world_y = np.array(Ground_traj.World_y)\n",
        "    Compar_traj_world_x = np.array(Compar_traj.World_x)\n",
        "    Compar_traj_world_y = np.array(Compar_traj.World_y)\n",
        "    X_RR = computeCorrelation(Ground_traj_world_x, Compar_traj_world_x)\n",
        "    Y_RR = computeCorrelation(Ground_traj_world_y, Compar_traj_world_y)\n",
        "    return  X_RR,Y_RR"
      ],
      "metadata": {
        "id": "rzdgi4j50Iji"
      },
      "id": "rzdgi4j50Iji",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import directed_hausdorff\n",
        "def Hausdorff(traj_ground,traj_compare):\n",
        "    ground_truth,Comparetraj = Samelen(traj_ground,traj_compare)\n",
        "    len_ground_truth = ground_truth.shape[0]\n",
        "    len_Compare_traj = Comparetraj.shape[0]     \n",
        "    num_traj_id = min(len_ground_truth,len_Compare_traj)    \n",
        "    Ground_traj = ground_truth[0:num_traj_id]    \n",
        "    Compar_traj = Comparetraj[0:num_traj_id]    \n",
        "    Ground_traj_world_XY = Ground_traj[['World_x','World_y']]\n",
        "    Compar_traj_world_xy = Compar_traj[['World_x','World_y']]\n",
        "    Dir_hausdorff_1 = directed_hausdorff(Ground_traj_world_XY, Compar_traj_world_xy)[0]\n",
        "    Dir_hausdorff_2 = directed_hausdorff(Compar_traj_world_xy, Ground_traj_world_XY)[0]\n",
        "    Dir_hausdorff_3 = directed_hausdorff(Compar_traj_world_xy, Ground_traj_world_XY)[1:]\n",
        "    Hausdorff_dis =  max(directed_hausdorff(Ground_traj_world_XY, Compar_traj_world_xy)[0], directed_hausdorff(Compar_traj_world_xy, Ground_traj_world_XY)[0])\n",
        "    return Hausdorff_dis"
      ],
      "metadata": {
        "id": "EF0Ccr__0RBQ"
      },
      "id": "EF0Ccr__0RBQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 列出你云盘里有啥\n",
        "!ls \"/content/drive/My Drive/data_set/Yolodata\""
      ],
      "metadata": {
        "id": "VvUi1a9f7Qcy"
      },
      "id": "VvUi1a9f7Qcy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ebike_traj_g = G_TRAJ_T[G_TRAJ_T['Vehicle_type']==22]"
      ],
      "metadata": {
        "id": "FQvWnXjW7xdj"
      },
      "id": "FQvWnXjW7xdj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gound_yolo_ID_high"
      ],
      "metadata": {
        "id": "JnvzJtbNF9OR"
      },
      "id": "JnvzJtbNF9OR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 对比轨迹"
      ],
      "metadata": {
        "id": "SEqtu8uj0dWJ"
      },
      "id": "SEqtu8uj0dWJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ground truth\n",
        "Groundtruthebike = Ebike_traj_g\n",
        "\n",
        "# Yolov7 轨迹数据\n",
        "yolov7data = r'/content/drive/My Drive/data_set/Yolodata/S_raw.csv'\n",
        "Yolov7_traj_orginal = pd.read_csv(yolov7data)\n",
        "Yolov7_traj_orginal = pd.DataFrame(Yolov7_traj_orginal)\n",
        "Yolov7_org = Yolov7_traj_orginal[Yolov7_traj_orginal.Vehicle_type==22]\n",
        "\n",
        "\n",
        "# 导入轨迹对应轨迹的ID匹配表\n",
        "for id in range(len(gound_yolo_ID_high)):\n",
        "    ground_id = gound_yolo_ID_high.ground_ID[id]\n",
        "    yolo_id = gound_yolo_ID_high.yolo_ID[id]\n",
        "    # 真实轨迹数据 \n",
        "    traj_ground1 = Groundtruthebike[Groundtruthebike.Object_id ==ground_id]\n",
        "    traj_ground0 = pd.DataFrame(traj_ground1)\n",
        "    \n",
        "    # CV输出的轨迹数据\n",
        "    traj_compare1 = Yolov7_org[Yolov7_org.Object_id == yolo_id]\n",
        "    traj_compare0 = pd.DataFrame(traj_compare1)\n",
        "\n",
        "    if traj_compare0.shape[0]>=10:\n",
        "\n",
        "        traj_ground,traj_compare = Alignment(traj_ground0,traj_compare0)\n",
        "\n",
        "        #  显示二维平面轨迹\n",
        "        fig1 = Trajmap(traj_ground,traj_compare)  \n",
        "#         SNRRR = SNR(traj_ground,traj_compare)\n",
        "#         print(SNRRR)\n",
        "#         R = RR(traj_ground,traj_compare)\n",
        "#         X_RR,Y_RR = RRR(traj_ground,traj_compare)\n",
        "#         print(X_RR,Y_RR)\n",
        "#         Hausdorffdis = Hausdorff(traj_ground,traj_compare)\n",
        "#         print(Hausdorffdis)\n",
        "        #  显示时间速度对比图像   \n",
        "#         fig2 = Timespeed(traj_ground,traj_compare) \n",
        "        #   显示时间加速度对比图像\n",
        "#         fig3 = Timeacc(traj_ground,traj_compare) \n",
        "        #  显示时间加加速度对比图像\n",
        "#         fig4 = Timejerk(traj_ground,traj_compare)"
      ],
      "metadata": {
        "id": "rE8NGC2E0gRU"
      },
      "id": "rE8NGC2E0gRU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 计算轨迹之间的差异指标\n",
        "##### 将轨迹数据分为无交互、弱交互和强交互三个阶段，分别比较不同类型轨迹的RMSE\n",
        "##### 第二步，对比cv输出轨迹与真实轨迹之间的差异，选取强交互情景下的轨迹做对比分析：\n",
        "##### 第三部分：对比降噪算法应用之后的轨迹\n",
        "##### 第一步，对比整体参数，\n",
        "##### 第二步，计算微观RMSE、DTWD等检验指标"
      ],
      "metadata": {
        "id": "-j6G0PIm0lhE"
      },
      "id": "-j6G0PIm0lhE"
    },
    {
      "cell_type": "code",
      "source": [
        "# 列出你云盘里有啥\n",
        "!ls \"/content/drive/My Drive/data_set/result_compare/\""
      ],
      "metadata": {
        "id": "QNPWZpBSDKck"
      },
      "id": "QNPWZpBSDKck",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 导入对应id数据\n",
        "# 导入高低密度下的车辆对应id列表\n",
        "# 导入ground turth dataset\n",
        "pathG_Y_ID = r'/content/drive/My Drive/data_set/ID_MATCH/YOLOV7_GROUND/high.csv'\n",
        "G_Y_ID_high = pd.read_csv(pathG_Y_ID)\n",
        "gound_yolo_ID_high = pd.DataFrame(G_Y_ID_high) #swatch the dict to dataframe\n",
        "# print(gound_yolo_ID_high)\n",
        "# 导入低密度下的车辆对应id列表\n",
        "# 导入ground turth dataset\n",
        "pathG_Y_ID = r'/content/drive/My Drive/data_set/ID_MATCH/YOLOV7_GROUND/low.csv'\n",
        "G_Y_ID_low = pd.read_csv(pathG_Y_ID)\n",
        "gound_yolo_ID_low = pd.DataFrame(G_Y_ID_low) #swatch the dict to dataframe\n",
        "# print(gound_yolo_ID_low)\n",
        "## 导入ground_truth and Mask_RCNN对应id数据\n",
        "# 导入低密度下的车辆对应id的列表\n",
        "pathG_M_ID = r'/content/drive/My Drive/data_set/ID_MATCH/Mask_Ground/low.csv'\n",
        "G_M_ID_LOW = pd.read_csv(pathG_M_ID)\n",
        "gound_mask_ID_LOW = pd.DataFrame(G_M_ID_LOW) #swatch the dict to dataframe\n",
        "# 导入高密度下的车辆对应id列表\n",
        "# 导入ground turth dataset\n",
        "pathG_m_HID = r'/content/drive/My Drive/data_set/ID_MATCH/Mask_Ground/high.csv'\n",
        "G_M_ID_HIHG = pd.read_csv(pathG_m_HID)\n",
        "gound_mask_ID_high = pd.DataFrame(G_M_ID_HIHG) #swatch the dict to dataframe\n",
        "\n",
        "# 进行敏感性分析\n",
        "marginal_Y_ID = r'/content/drive/My Drive/data_set/ID_MATCH/YOLOV7_GROUND/all.csv'\n",
        "marginalmx = pd.read_csv(marginal_Y_ID)\n",
        "marginalmx_ID_ALL = pd.DataFrame(marginalmx) #swatch the dict to dataframe"
      ],
      "metadata": {
        "id": "dwJvWaB60mrr"
      },
      "id": "dwJvWaB60mrr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 计算差异指标的表格\n",
        "#### 是否可以将所有的表格汇集到一个表格中得到最后的表格呢？"
      ],
      "metadata": {
        "id": "WeXqFC5h0z9s"
      },
      "id": "WeXqFC5h0z9s"
    },
    {
      "cell_type": "code",
      "source": [
        "# compar_data\n",
        "compar_data = r'/content/drive/My Drive/data_set/Yolodata/S_raw.csv'\n",
        "\n",
        "\n",
        "compar_traj= pd.read_csv(compar_data)\n",
        "compar_traj = pd.DataFrame(compar_traj)\n",
        "compar_traj_ebike = compar_traj[compar_traj.Vehicle_type==22]\n",
        "# 计算多个loop对应轨迹的RMSE\n",
        "\n",
        "gound_yolo_ID_high = gound_yolo_ID_low\n",
        "hang = gound_yolo_ID_high.shape[0]\n",
        "LoopRMSE = np.zeros((hang,4))\n",
        "LoopSNR = np.zeros((hang,4))\n",
        "LoopHausdorff = np.zeros((hang,4))\n",
        "LoopRRsquared_x = np.zeros((hang,4))\n",
        "LoopRRsquared_y = np.zeros((hang,4))\n",
        "\n",
        "RMSE_allloop = pd.DataFrame(LoopRMSE)\n",
        "SNR_allloop = pd.DataFrame(LoopSNR)\n",
        "Hausdorff_allloop = pd.DataFrame(LoopHausdorff)\n",
        "Rsquared_allloop_x = pd.DataFrame(LoopRRsquared_x)\n",
        "Rsquared_allloop_y = pd.DataFrame(LoopRRsquared_y)\n",
        "\n",
        "for loop in range(0,1):\n",
        "    Rmse_list =[]\n",
        "    SNR_list = []\n",
        "    Hausdorff_list = []\n",
        "    Rsquared_x_list = []\n",
        "    Rsquared_y_list = []\n",
        "    for id in range(len(gound_yolo_ID_high)):      \n",
        "        ground_id = gound_yolo_ID_high.ground_ID[id]\n",
        "        yolo_id = gound_yolo_ID_high.yolo_ID[id]\n",
        "            # 真实轨迹数据 \n",
        "        traj_ground0 = Groundtruthebike[Groundtruthebike.Object_id ==ground_id]\n",
        "        traj_ground0 = pd.DataFrame(traj_ground0)\n",
        "            # CV输出的轨迹数据\n",
        "#         CV_traj_loops = Yolov7_wt[Yolov7_wt.loopid ==loop]   \n",
        "    \n",
        "        CV_traj_loops = compar_traj_ebike\n",
        "        traj_compare0 = CV_traj_loops[CV_traj_loops.Object_id == yolo_id]\n",
        "\n",
        "#         mask_traj_loops = Mask_ebike_sfpf[Mask_ebike_sfpf.loopid == loop]\n",
        "#         traj_compare0 = mask_traj_loops[mask_traj_loops.Object_id == yolo_id]\n",
        "    #         print(traj_ground0)\n",
        "    #         print(traj_ground0.World_y)\n",
        "    #         min(pd.array(Groundtruthebike.World_y))\n",
        "        if traj_compare0.shape[0]>=10:\n",
        "            traj_ground,traj_compare = Alignment(traj_ground0,traj_compare0)\n",
        "\n",
        "                # 提取轨迹数据\n",
        "# # #       显示二维平面轨迹\n",
        "            # fig1 = Trajmap(traj_ground,traj_compare)  \n",
        "# #         显示时间速度对比图像   \n",
        "#             fig2 = Timespeed(traj_ground,traj_compare) \n",
        "# #         显示时间加速度对比图像\n",
        "#             fig3 = Timeacc(traj_ground,traj_compare) \n",
        "# #         显示时间加加速度对比图像\n",
        "#             fig4 = Timejerk(traj_ground,traj_compare)\n",
        "            \n",
        "            RMSE = Index_rmse(traj_ground,traj_compare)\n",
        "            Rmse_list.append(RMSE)\n",
        "        \n",
        "            # SNRRR = SNR(traj_ground,traj_compare)\n",
        "            SNRRR = SNR(traj_ground,traj_compare)\n",
        "            SNR_list.append(SNRRR) \n",
        "\n",
        "            HausdorffR = Hausdorff(traj_ground,traj_compare)\n",
        "            Hausdorff_list.append(HausdorffR)\n",
        "        \n",
        "            #R-squared value\n",
        "            Rsquared_x,Rsquared_y = RRR(traj_ground,traj_compare)\n",
        "            Rsquared_x_list.append(Rsquared_x)\n",
        "            Rsquared_y_list.append(Rsquared_y)\n",
        "        \n",
        "    A = pd.DataFrame(Rmse_list)\n",
        "    RMSE_allloop.iloc[:,loop] = A\n",
        "    B = pd.DataFrame(SNR_list)\n",
        "    SNR_allloop.iloc[:,loop] = B\n",
        "    C = pd.DataFrame(Hausdorff_list)\n",
        "    Hausdorff_allloop.iloc[:,loop] = C\n",
        "    RX = pd.DataFrame(Rsquared_x_list)\n",
        "    Rsquared_allloop_x.iloc[:,loop] = RX\n",
        "    RY = pd.DataFrame(Rsquared_y_list)\n",
        "    Rsquared_allloop_y.iloc[:,loop] = RY\n",
        "   \n",
        "RMSE_allloop.iloc[:,1:3]=gound_yolo_ID_high.iloc[:,0:2]\n",
        "SNR_allloop.iloc[:,1:3]=gound_yolo_ID_high.iloc[:,0:2]\n",
        "Hausdorff_allloop.iloc[:,1:3]=gound_yolo_ID_high.iloc[:,0:2]\n",
        "Rsquared_allloop_x.iloc[:,1:3]=gound_yolo_ID_high.iloc[:,0:2]\n",
        "Rsquared_allloop_y.iloc[:,1:3]=gound_yolo_ID_high.iloc[:,0:2]\n",
        "\n",
        "# print(gound_yolo_ID_high)\n",
        "filepath1 = r\"/content/drive/My Drive/data_set/result_compare/G_Y_H_RMSE.csv\"\n",
        "RMSE_allloop.to_csv(filepath1, index=False, mode='a+', header=False)\n",
        "filepath2 = r\"/content/drive/My Drive/data_set/result_compare/G_Y_H_SNR.csv\"\n",
        "SNR_allloop.to_csv(filepath2, index=False, mode='a+', header=False)\n",
        "filepath3 = r\"/content/drive/My Drive/data_set/result_compare/G_Y_H_Hausdorrf.csv\"\n",
        "Hausdorff_allloop.to_csv(filepath3, index=False, mode='a+', header=False)\n",
        "filepath4 = r\"/content/drive/My Drive/data_set/result_compare/G_Y_H_Rsquared_X.csv\"\n",
        "Rsquared_allloop_x.to_csv(filepath4, index=False, mode='a+', header=False)\n",
        "filepath5 = r\"/content/drive/My Drive/data_set/result_compare/G_Y_H_Rsquared_Y.csv\"\n",
        "Rsquared_allloop_y.to_csv(filepath5, index=False, mode='a+', header=False)"
      ],
      "metadata": {
        "id": "Yho0I9IJ06SN"
      },
      "id": "Yho0I9IJ06SN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}